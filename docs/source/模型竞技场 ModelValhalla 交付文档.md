# 模型竞技场 ModelValhalla 交付文档

> 姚润茂 方钧同 李科霖 钟健坤

[TOC]

## 交付产品

### 产品访问方式



### 仓库链接

Github仓库：https://github.com/cre185/Model_Valhalla

## 产品目标
为大模型提供一个开放、公平、可靠的评测机会，为人们直观展示模型水平与排行，从而进一步促进模型的发展和创新。
### 功能目标
#### 1. 用户身份验证

   用户在登录以及注册页面可以完成用户注册、用户登录两项操作，在个人中心可以完成查看个人信息、修改个人信息、修改安全信息三项操作。管理员可以执行管理员注册、管理员登录两项操作。

   1.1 用户注册

   用户点击注册按钮，需要先填写手机号和密码，之后点击“获取验证码”，输入手机上收到的验证码，验证码每1分钟可以发送一次。完成验证后勾选左下角的“已阅读并同意《服务协议》和《隐私政策》”，点击“注册”按钮，系统会提示注册成功，并返回到登录页面。考虑到对用户数据和隐私的保护，系统会建议用户使用强密码，并且要求用户填写电子邮箱地址以找回密码。

   1.2 用户登录

   登录应考虑多因素身份验证，提供多种登录方式。用户可以选择输入手机号与密码验证身份，也可以通过进行手机验证码验证，两种测试方式可以通过点击窗口上的相应选项进行切换。无论采取哪种方式，填写表单项后用户点击“登录”，如果信息正确将进入到测试主页。

   1.3 查看个人信息

   用户注册后信息可以随时查看自己的信息，点击“个人中心”会弹出下拉框，点击“个人信息”将跳转到个人信息页，该页面将展示用户名、用户类型等基本信息。此外，还可以看到用户关注的模型、数据集以及用户的测试记录。用户通过点击个人页面的部分选项也能直接跳转至相关页面，如页面默认展示部分测试记录，用户可以点击“查看更多”选项跳转至测试记录页面查看完整记录。

   1.4 修改个人信息

   在“用户设置”页面，用户能看到除部分安全信息外的所有各项信息，并可以直接修改。初始部分信息可能不存在，用户可以在各信息标签处通过点击或输入等方式添加或修改头像、用户名，也可以在下方的“基础信息”选项修改国家、个人简介等信息，修改完成后点击保存即可。

   1.5 修改安全信息

   用户选择“安全设置”选项会展示修改安全信息界面，其中展示用户的私密信息如绑定手机号、绑定邮箱等，手机号中部分会以“*”遮挡，保护用户隐私。在该页面修改私密信息如密码时，将需要输入原本密码以验证身份，或是点击“忘记密码”按钮跳转至验证码重置密码界面，正确输入验证码后方可设置新密码。

   **以下为管理员可以额外执行的操作。**

   1.6 管理员注册

   除了普通用户外，还要有少量的管理员用户。在登录页面点击“我是管理员”进入管理员登录界面，点击“注册账号”，除了填写基本信息外，还需要填写由开发人员提供的邀请码，信息正确后，点击“注册”，系统会弹出注册成功并返回管理员登录页面。

   1.7. 管理员登录

   管理员登录的过程类似用户登录的过程，首先选择使用账密登录或是手机验证码登录的方式，在页面上的表单中输入正确的管理员账号信息后，点击“登录”即可进入主页。平台会判断该账号为管理员账号，并在浏览过程中解锁管理员权限，能执行各种管理员操作。

#### 2. 数据集维护

   用户在数据集页面可以完成查看数据集列表、查看详细信息、数据集筛选和排序、数据集下载、数据集上传、数据集反馈、数据集举报七种操作。管理员可以执行数据集修改、数据集删除、反馈与举报处理三种操作。

   2.1 查看数据集列表

   用户可以在页面上直观地看到现有数据集的列表以及各数据集的基本信息，如数据集编号、名称、创建时间等。数据集列表支持按词条排序的功能，通过点击数据集列表上方的词条可以切换按该词条排序/逆序展示数据集列表，下方列表将随之更新。

   2.2 查看详细信息

   用户点击某一数据集的“查看”选项将会展开页面显示该数据集的更详细的信息，该详细信息分为若干栏，包含详细资料栏展示如发布方、应用领域等信息、测试表现栏展示该数据集在平台上进行测试的表现等。用户可以通过点击展开页面上方的标签切换显示内容。在预览界面还可以查看数据集的部分内容。

   2.3 数据集筛选与排序

   页面上方有针对数据集列表的筛选框，用户可以在筛选框中按不同条件筛选出想要的数据集，如按编号、名称等关键词条进行检索，或是选取创建时间区间等操作。选择完成后点击“检索”按钮，下方数据集列表也会随之更新。此外，用户还能根据创建时间、编号等对数据集排序。

   2.4 数据集下载

   用户可以通过点击“下载”按钮下载指定数据集，也可以通过勾选数据集并点击“批量下载”按钮下载多个数据集。

   2.5 数据集上传

   用户可以通过点击“上传”按钮展开上传数据集界面，填写基本信息如名称、应用领域、标签，上传格式正确的数据集文件后即可完成数据集上传。系统会通知管理员新数据集被上传，管理员审查数据集，存在例如信息不完整、数据集文件错误等情况的数据集会被删除。而合法的数据集将被存储在平台上，能够参与平台上的模型的能力测试并出现在排行榜上。

   2.6 数据集反馈

   用户点击某一数据集的“反馈”选项将会弹出数据集反馈界面，用户可以从反馈类型标签中点击并选择自己本次反馈的类型，如数据错误、存在更新版本等，并在下方的输入框中写下自己具体的想法，从而对该数据集提出自己的意见，也可以通过“上传文件”按钮附上相关文件。该反馈信息可以被管理员查看，并可以得到管理员的处理。

   2.7 数据集举报

   用户发现某一数据集出现异常或具有很严重的问题时，可以点击“反馈”选项之后选择上方的“举报”选择框进入举报界面，在该界面中选择举报理由以及详细描述，随后举报也将发送至管理员进行处理，且管理员应当优先进行举报的处理。

   **以下为管理员可以额外执行的操作。**

   2.8 数据集修改

   管理员可以针对数据集的属性进行修改，在管理员视角可以选择“修改”选项，之后进入修改页面，可以对数据集的大部分属性进行勘误，如纠正来自数据集提供者的错误等。

   2.9 数据集删除

   管理员发现不合格/错误的数据集后可以删除数据集，在管理员视角选择“删除”选项，会弹窗提示是否确认删除，防止管理员误操作错误删除数据集。如若确实需要删除，选择“确定”即可。

   2.10 反馈与举报处理

   管理员在个人中心或右上角的通知框可以查看到数据集的反馈与举报消息，点击后可以跳转到对应的数据集，之后根据用户的反馈或者举报的内容，可以选择是否对数据集进行修改或删除。

#### 3. 模型排行榜功能

   用户在模型排行榜界面可以完成查看模型排名、对抗评测记录查看、模型详细信息、数据集表现、排行榜下载五种操作。

   3.1 查看模型排名

   排行榜可以选择按多种标准对模型水平进行排名并按序显示，可以针对数据集综合得分以及对抗得分等进行排序。通过点击排行榜上方的相应词条即可选择按该词条顺序/逆序排列，点击后排行榜将实时更新。也可以搜索具体的模型查看其详细情况。

   3.2 模型详细信息

   用户可以点击某一模型的“查看”选项弹出模型详情界面展示详细信息，该详细信息页面类似数据集详细信息页面，同样分为若干栏，如详细资料栏展示发布方、应用领域等信息、测试表现栏展示该模型在平台上的测试表现等。用户可以通过点击展开页面上方的标签切换显示内容。

   3.3 对抗评测记录查看

   平台会记录对抗评测的结果并用于计算模型的Elo值，后者表示一个模型的相对水平，分值越高水平越高。Elo值也是可选的模型排序选项，用户在点击详情后面后，在对抗记录页面可以看到模型参与的对抗评测的信息以及胜负情况，点击其中的“+”按钮，则可以看到对抗评测的问题以及双方的回复。

   3.4 排行榜下载

   用户可以点击“下载”按钮对排行榜进行下载，下载可以将排行榜保存至用户本地，方便随时查看，并能应用已有的样式调整。表格能够以.csv的文件格式存储数据，从而能在Excel中打开。以下为管理员可以额外执行的操作。

   3.5 数据集表现

   点击模型详情后进入数据集表现页面可以看到模型在各数据集上的具体得分。此外，还可以通过数据集名称、数据集容量大小等多种要素对数据集筛选，以查看模型在某些特定数据集下的表现。

#### 4. 模型测试

   用户在模型测试界面可以完成对抗评测和主观评测、测试结果与反馈三种操作。

   4.1 对抗评测

   用户在测评时可以选择参与对抗评测，平台随后将会在界面上展示对抗评测界面。用户可以阅读界面上方的对抗评测说明与规则，在下方的对抗选择界面进行操作，如在输入框输入自己的问题或选择平台提供的问题，观察下方输出框中两个模型的结果，并通过点击对应按钮的方式做出判断。对抗评测进行时，平台将按照基于Elo值的匹配机制选择进行对抗的模型，并基于结果更新Elo值。对抗评测和下方自动评测均结束后将跳转至答案评估界面。

   4.2 主观评测

   用户可以进行主观评测，选择完模型与数据集后，平台通过与数据集提供的标准答案比对的方式进行判断正误，过程中平台将记录得分、正确率等相关信息，用于测试结束后的展示。主观评测和上方对抗评测均结束后将跳转至测试结果界面。

   4.3 测试结果与反馈

   用户在完成对抗评测后可以选择提供反馈，点击反馈后面后填写反馈意见，可以是评测机制的建议也可以是对平台提供问题的补充，提交后将会通知系统管理员，管理员会进行相应的维护与改进。

#### 5. 用户间交流

   用户可以在讨论区发表自己的意见，查看其他用户的讨论，并能收到他人对自己的点赞以及评论等消息。

   5.1 讨论区功能

   在数据集详情页面以及模型详情页面均有讨论区，用户在遵守社区守则的前提下可以对模型或者数据集进行讨论或者点评，并且也能和其他用户互动。

   5.2 站内信箱

   用户在讨论区的评论被他人评论或者点赞后，会接收到相应的消息，在个人中心或者页面右上角的通知按钮点击具体消息，会跳转到对应的讨论区。


### 性能要求

#### 1. 任务调度与处理任务队列：

   由于大模型生成任务通常耗时较长，可以使用任务队列的方式管理任务，比如使用高吞吐量的消息队列系统或任务队列框架来避免请求长时间阻塞，确保任务可以按序进行处理。并行处理：支持多任务并行处理，以提高测试效率。使用多线程、多进程或分布式计算等技术，确保系统能够同时处理多个测试任务。

#### 2. 数据处理与传输分页技术：

   由于文本数据集通常较大，采用前后端分页策略。后端分页可以降低网络传输延迟，将数据按需发送到前端。前端分页可以减少页面渲染开销，确保用户在浏览数据时有良好的体验。

#### 3. 数据更新与排行榜局部更新：

   当数据集新增条目或个别条目更新时，只对新增或修改的数据进行处理和测试。避免对整个数据集进行全量测试，以降低系统开销。排行榜更新：定期更新排行榜结果。在有新的测试结果时，及时更新排行榜信息，确保用户获取到最新的模型性能排名。

#### 4. 较低的响应时间

用户在页面间进行跳转，以及进行评测等涉及到与后端进行交互的操作，如搜索、获取信息时，页面都应维持在较低的响应时间，以优化用户体验。由于项目采用前后端分离，某些功能如评测、讨论等可以采用异步实现，先在前端响应，再与后端数据库进行交互，从而极大地缩短用户的等待时间。

#### 5. 较高的吞吐量

页面应该能同时处理数百个请求，能支持大量用户同时在线并进行操作，在较高的并发量下，后端的各接口响应时间也应该不高于1s。

### 预期指标

+ 用户登录响应时间应该不多于2s。

+ 用户查看模型排行榜以及数据集的信息的等待时间应该不高于1s，筛选以及排序的等待时间应该不多于100ms。

+ 用户对排行榜以及数据集进行检索、筛选的响应时间应该不多于100ms。

+ 用户上传文件（10M以下）的等待时间应该不多于1s。

+ 涉及到讨论区的操作用户的等待时间应该不多于50ms。

+ 进行评测时，评测开始到模型开始输出的等待时间应该不多于1s，且模型输出成功的概率应该不低于95%。

+ 用户跳转页面的等待时间应该不多于2s。

## 开发组织管理

### 过程管理

项目开发主要开始于第七周，整体开发工作基本按照项目初期的指定的开发时间线进行，部分功能点的开发顺序因实际需要做了一定调整。具体开发时间线如下：

- 7-8周完成开发环境的搭建，创建前后端通信连接，并完成用户注册和登录功能（包括用户中心的个人信息及安全信息修改）；
- 9-10周完成主要页面排行榜相关功能的开发，包括排序、筛选、查看模型详情等功能，此外还完成了对抗评测的初步版本；
- 11周完善了对抗评测，补全了另一功能主管评测，并对已有功能进行了一定的bug修复及UI优化；
- 12-13周完成了数据页面相关功能的开发，在排行榜功能基础上增加了用户自定义标签和模型表现可视化等特色功能
- 14-15周集中解决了尚未完成的任务：在功能方面完成了用户个人中心和站内信的相关实现，测试方面主要集中于性能测试，并对已有功能进行了bug修复和UI优化。在这一阶段中，同时完成了部署的相关工作。

在开发的过程中，虽然因为计划外功能的加入以及组员身体原因等因素，总体开发的完成稍慢与原定计划，但由于组员之间积极沟通，任何与开发进度相关的变更都会在集中开发会议中进行商讨，因此开发计划的跟进始终在可控的范围内。此外，我们也在迭代过程中积极听取助教团队的意见，并与之进行了充分的沟通讨论，在补足功能的同时，也在分支管理这一我们相对不熟悉的领域进行了较大的改善。从最初的所有人共享一个开发分支，到各组员独立的开发分支，再到基于功能点的开发分支，项目的分支管理逐渐趋于规范化。

总体的开发迭代共分三轮，第一轮迭代为6-8周，完成了用户注册和登录功能后的应用初具雏形，此为发布的版本一；9-11周完成了核心功能排行榜和测试，应用具备了作为开放大语言模型能力排行榜的基本功能，此为发布的版本二；12-13周完善了数据集功能并完成了bug修复和UI优化，此时的应用已经可以正常使用，此为发布的版本三；14-15周对拓展功能进行了实现，完成了性能测试和部署的相关工作，此为发布的最终版本。

### 人员分工

姚润茂（前端）：设计网页的主页；设计用户设置页面并实现用户信息修改，实现登出功能；实现排行榜的下载，设计并实现排行榜页面中模型详情部分的详细信息、讨论区；设计并修改部分对抗评测页面，编写对抗评测的相关接口；设计并实现数据集页面中数据集详情部分的详细资料、预览、测试表现、讨论区，以及管理员模式下对数据集的详情页的修改；实现中英文。

方钧同（前端）：

李科霖（前端）：用户注册部分校验规则的接口编写；数据集表现页面；编写与完善对抗评测页面；编写对抗评测逻辑以及交互功能；实现数据集上传页面；实现数据集反馈页面；实现数据集举报页面。编写站内信功能，修改讨论区中点赞、评论等功能的具体实现，完成向后端的信息转发。实现文件上传功能。

钟健坤（后端）：

### 开发环境

#### 前端：

前端主要使用Vue.js，通过组件化思想， 将页面拆分为独立组件，方便维护和服用。 通过 Vue 的数据绑定机制和指令系统，我们可以简化 DOM 操作，提高团队的开发效率。

项目使用了字节开发的Arco Design库，我们利用其提供的丰富组件，实现了较为美观实用的UI界面。此外，对于其中一些不能达到我们需求的组件，我们也对齐进行了一定的调整，并最终应用于我们的项目中。

项目整体采用npm作为包管理工具，路由方面的实现通过Vue.router实现。请求的发送通过axios实现，其强大的interceptor功能便于我们对数据包进行统一的前处理和后处理。存储管理使用了pinia，其模块化的特点以及响应式的api极大程度地便利了整体的开发。此外，使用的库还包括绘制图标的chart.js，统一时间管理机制的mitt以及对数据集(.csv)进行处理的papaparse。

#### 后端：

后端使用django框架及其扩展包django-rest-framework，实现了对前端的数据请求的响应。通过django框架的ORM机制，可以轻松实现对数据库的操作。 
DRF框架提供了一套强大的序列化工具以及视图类，保证接口RESTful风格，同时也极大的简化了开发难度。 
项目使用pip和conda作为包管理工具，借助PyJwt实现了用户登录的token验证，使用了Django-cors-headers解决了跨域问题。DRF内置的更强大的Response对象让后端返回的响应更加灵活，APIClient测试工具则使得测试工作更加便捷。  

### 配置管理

项目的版本控制和变更管理主要通过Git实现。

分支管理在进行过两次优化之后，主要采用基于功能的分支管理策略，具体如下：

- 项目的后端由于仅由钟健坤同学开发，因此独占backend分支。前端的主分支为main。
- 每当需要开发新的主要功能点时，从main分支拉取新分支并命名为对应的功能点（如ranking）。
- 小组成员分别开发主要功能点的具体功能时，需从主功能点拉取新分支并命名（如ranking_details），开发完成后合并到主要功能点分支。
- 主要功能点所有具体功能开发完成后，对这一功能点的相关内容进行测试，确认无误后合并到主分支。

代码审查主要通过eslint进行了规范性检查，具体可参照代码编写规范文档。

我们对代码进行的自动化测试主要为单元测试，具体的测例编写随功能开发同步进行，最终覆盖率可达97%。

版本号主要依据最初指定的开发时间线确定，具体格式为`x.y.z`，其中x为主要版本号，可参阅过程管理部分的内容；y为次版本号，记载了当前主版本中已经完成的主要功能点的个数；z则是修订号，在对当前版本进行bug修复和UI优化后会进行更新。

## 系统设计

### 前端交互

#### 界面设计原则

1. **一致性：** 前端界面的布局基本都采用上下结构，页面上部分是简单组件，页面中下部分是主要组件，界面元素的设计在整个应用程序中保持一致，这样更有助于用户理解和掌握我们网页的使用方式。
2. **可用性：** 我们的界面设计简单直观，基本都是简单直接的按钮操作，这样使得用户能够轻松、有效地完成任务。
3. **反馈性：** 在用户执行一些操作时，我们会通过界面的反馈及时告知用户操作的结果，例如“登录成功”、“登录过期”等全局提醒，这些反馈可以帮助用户了解其行为的后果。
4. **可导航性：** 我们在界面侧边提供清晰、一致的导航结构，使用户能够轻松找到所需的信息和功能，有助于用户快速了解网页的结构和布局。
5. **美观性：** 我们尽量使界面外观吸引人，如考虑一致美观的颜色搭配、字体选择、图标设计等，以创造愉悦的用户体验，并且我们没有位了一味追求美观而牺牲功能性。
6. **普适性**：网页支持中英文双语，可以服务使用不同语种的用户群体。

#### 用户交互流程

*写在开头：由于阿里云短信API的特殊性，只能给5个已绑定的手机号发送短信验证码，4位开发人员已经占用了4个位置，如果某些功能涉及到短信验证码，请联系姚润茂（微信：18810129120）绑定手机号。*

##### 主页

![1](./images/1.png)

+ 页面最上方是标题栏，标题栏左侧是项目名称，标题栏右侧依次是**切换语言按钮**、**切换暗黑/明亮模式按钮**、**用户状态**。未登录状态下用户可以通过点击主页的**Get Started**或标题栏的**您好，请先登录**进行登录。

![1](./images/7.png)

+ 用户登录后标题栏会额外出现一个**站内信按钮**。登录后点击主页的**Get Started**会跳转到排行榜页面。
+ 页面侧边是导航栏，从上到下依次是**主页**、**排行榜**、**数据集**、**评测（包括主观评测、对抗评测）**、**个人中心（包括用户信息、用户设置）**，用户可以点击对应的选项卡跳转到相应的页面。在导航栏的右下角有一个**最小化按钮**，点击即可缩小导航栏，再次点击可以放大导航栏。

![1](./images/8.png)

+ 鼠标悬浮在标题栏的**用户头像**上，会弹出下拉菜单，点击**用户信息**可以跳转到用户信息页，点击**用户设置**可以跳转到用户设置页，点击**退出登录**可以退出登录。

##### 登录页

![1](./images/2.png)

![1](./images/3.png)

+ 左侧是图片轮播组件，简单介绍网页的功能。
+ 右侧是登录组件，分为**密码登录**和**验证码登录**，前者输入账号和密码，后者输入手机号和短信验证码，之后点击**登录**按钮即可登录，如没有账号可以点击登录按钮下方的**注册账号**按钮跳转到注册页。

##### 注册页

![1](./images/4.png)

+ 用户账号分为两种——**普通用户**和**管理员**。

![1](./images/5.png)

+ 普通用户填写相应字段后点击**注册按钮**即可完成账号注册。

![1](./images/6.png)

+ 管理员相比普通用户要额外填写**管理员邀请码**，完成填写后点击**注册按钮**即可完成账号注册。

##### 排行榜

![1](./images/9.png)

+ 排行榜页面有一个欢迎页，可以通过**鼠标下滑**或点击**Get Started**进入排行榜主页

![1](./images/10.png)

+ 页面左上方是搜索框，输入关键字后点击**搜索按钮**即可搜索模型。点击右上方的**下载**即可以`csv`格式下载排行榜。点击右上方的**齿轮**后在下拉菜单中可以选择排行榜展示的信息条目，可额外显示所有模型在特定数据集上的得分，在下拉菜单中**上下拖动**菜单项可以调整显示顺序。
+ 页面中间是排行榜，点击任何一个表头都可以**重新排序**。每个模型右侧都有**查看**按钮，点击可以查看模型详情。

![1](./images/11.png)

+ 模型详情页包括四个标签页：**详细信息**、**数据集表现**、**对抗记录**、**讨论区**，点击模型详情页上方的**关注**即可关注该模型。
+ 在详细信息标签页可查看模型的基本信息。

![1](./images/12.png)

+ 在数据集表现标签页可查看模型在不同数据集上的得分，可以通过上方的**筛选框**筛选数据集。

![1](./images/13.png)

+ 在对抗记录标签页可查看与该模型相关的对抗评测记录，可以通过上方的**筛选框**筛选评测记录，可以点击每条记录前对抗详情栏的**+**来查看详细记录。

![1](./images/14.png)

+ 在讨论区标签页可以**发表评论**、**点赞**、**点踩**、**回复评论**。

##### 数据集

![1](./images/15.png)

+ 数据集页上方是**筛选框**，输入筛选项后点击**搜索**可以筛选数据集，点击**重置**可以清楚筛选项。先**勾选**目标数据集，然后点击**批量下载**即可下载多个数据集文件。
+ 数据集页中间展示了现有数据集条目，点击任意**表头**都可以重新排序，点击**添加标签**可以为对应数据集添加标签，点击**下载**可以下载相应数据集文件，点击**查看**可以查看数据集详情。

![1](./images/30.png)

+ 管理员模式下数据集操作新增了**删除**。

![1](./images/16.png)

+ 点击数据集主页的**上传**可以上传数据集。

![1](./images/17.png)

+ 点击数据集主页的**反馈**可以反馈数据集相关的问题。

![1](./images/18.png)

+ 数据集详情页包括四个标签页：**详细资料**、**预览**、**测试表现**、**讨论区**，点击数据集详情页上方的**关注**即可关注该数据集。
+ 在详细资料标签页可以查看数据集详细资料。

![1](./images/19.png)

+ 在预览标签页可以预览一小部分数据集内容。

![1](./images/20.png)

+ 在测试表现标签页可以查看不同模型在该数据集上的得分，在右侧可视化部分可以**选择**三个模型，然后点击**确认**，即可查看得分可视化结果。

![1](./images/21.png)

+ 在讨论区标签页可以**发表评论**、**点赞**、**点踩**、**回复评论**。

![1](./images/29.png)

+ 管理员模式右上角的按钮由**下载和反馈**被替换为**下载和编辑**，数据集旁边的**关注**也被替换为**替换数据集文件**，点击**编辑**按钮后，可以修改详细资料的各个条目，也可以更换数据集文件。

##### 主观评测

![1](./images/22.png)

+ 主观评测页上方是主观评测规则，在页面下方分别**选择模型**、**选择主观数据集**后，点击**确认**，即可开始主观评测。

![1](./images/23.png)

+ 点击右下方的**生成答案**可以获取模型回复，在评分部分可以**打分**，评完一题可以点击**下一题**或**上一题**切换题目，评完所有题目后可以点击**提交结果**来提交本次的评分结果。

##### 对抗评测

![1](./images/24.png)

+ 对抗评测页上方是对抗评测规则，在页面下方**选择模型**后，点击**确认**，即可开始对抗评测。

![1](./images/25.png)

+ 可以在输入框**直接输入问题**，也可以通过**填充**来填入预设问题，点击**发送**即可提交问题并获取模型回复。

![1](./images/26.png)

+ 在评测后可以通过下方的**评价按钮**选择用户青睐的模型（需要注意的是一旦评价了模型，用户就不能再继续输入问题评测模型，只能开启新一轮评测），可以点击**重新生成结果**来让模型重新回复，可以点击**新一轮评测**来开启新一轮对抗评测，可以点击**反馈建议**来提交用户关于对抗评测页面的建议。

##### 用户信息

![1](./images/27.png)

+ 个人信息页面展示了用户关注的模型、关注的数据集、对抗评测记录、收到的信息，点击**查看更多**可以显示完整信息。

##### 用户设置

![1](./images/28.png)

+ 点击**用户头像**可以更换头像，点击每栏用户信息右边的**修改**可以修改对应用户信息。

![1](./images/31.png)

+ 修改密码，首先验证手机号，随后按照要求修改密码即可。

![1](./images/32.png)

+ 修改绑定手机号，首先验证当前绑定的手机号，随后按照要求修改即可。

![1](./images/33.png)

+ 绑定邮箱/修改已绑定的邮箱，首先验证绑定的手机号，随后按照要求修改即可。

### 后端模块
后端在django中分为了四个部分（app）：用户管理、数据集、排行榜、模型测试，主要对应我们的四个大功能部分。对于我们的项目而言，这四个部分并没有明显的主次之分，都可以算作核心模块。下面描述一下各模块的主要功能。  
#### 用户管理
用户管理模块主要负责用户的注册、登录等基本功能，一个较重要的功能是用户之间的站内信功能。后端将每一条消息作为一个对象，另外记录每条消息对应的发送者和接收者，从而解决了群发消息的问题。  
#### 数据集
数据集模块主要负责数据集的上传、下载、筛选等功能。数据集支持下载以及预览功能，这部分通过文件操作完成。  
#### 排行榜
排行榜模块主要负责排行榜的展示、筛选等功能。该模块记录了所有模型在数据集上进行测试得到的结果信息，并提供了接口用于按特定标准进行筛选获取。  
#### 模型测试
模型测试模块主要负责模型以及评测的功能。后端实现了模型在某一数据集上的自动评测，同时开放了对某一模型进行调用的接口，以便于前端进行人工评测。  
### 接口规范
由于使用了DRF框架，我们的接口规范主要遵循RESTful风格。 
REST即Representational State Transfer的缩写，提供了一组设计原则和约束条件。具体而言我们的接口满足：  

1. 以资源为基础：资源是REST架构中的核心概念，django框架中的Model刚好可以看作资源。  
2. 统一接口：对于资源的操作，我们使用统一的接口，即HTTP协议中的方法，包括`GET`、`POST`、`PATCH`、`DELETE`等。对于最基本的资源的增删改查操作，我们遵循使用`GET`方法获取资源，使用`POST`方法创建资源，使用`PATCH`方法部分更新资源，使用`DELETE`方法删除资源的规范。  
3. 无状态：对于每个请求，服务器不会记录任何状态信息，即每个请求都是独立的。  

### 数据库设计
由于我们使用的是MySQL数据库，因此数据库模式即为数据库本身。 
我们数据库中的表较多，下面边列举边介绍表之间的依赖关系。  

#### 用户管理模块
用户管理模块中的表包括：  
* user：用户表，记录了用户的基本信息，包括用户名、密码、邮箱、手机号、用户类型等。  
* verifyemail/verifymsg：验证码表，记录了用户需要发送验证码时的验证码信息，包括验证码、验证码类型、验证码过期时间等。  
* msg/msgtarget：站内信表，msg表用于记录信息本身，msgtarget表用于记录信息的所有接收者。  
* llmsubscription/datasetsubscription：关注表，记录了用户关注的模型和数据集。这两张表依赖于dataset和llm对象。  
#### 数据集模块
数据集部分仅有一张表，也即dataset表，记录了数据集的基本信息，包括数据集名称、数据集描述、数据集文件等。  
#### 排行榜模块
排行榜模块中的表包括：  
* credit：评测分数表，表示某一模型在某一数据集上的得分。这个对象对于每个数据集/模型的组合恰好存在一个。注意这个对象会在创建/删除模型/数据集时自动更新。  
* llmcomment/datasetcomment：评论表，记录了用户对模型/数据集的评论。评论这一功能由于在前端页面上是随着排行榜一起展示的，故在后端也分类在排行榜模块中。这一表依赖于user、dataset、llm三种对象。  
* llmlike/datasetlike：点赞表，记录了用户对模型/数据集的评论的点赞的点赞。需要user的信息。  
#### 模型测试模块  
模型测试模块中的表包括：  
* llms：模型表，记录了模型的基本信息，包括模型名称、模型描述、模型url等。  
* battlehistory：对抗记录表，记录了不同模型之间的对抗评测记录。依赖于两个不同的llm对象。  
### 其他模块
后端的其他模块和功能均放在了utils文件夹下，目前该文件夹包含如下的若干个功能：  
* jwt/admin_required：采用jwt进行了用户的身份验证，我们的jwt包含了用户的id以及用户是否为管理员的信息。这两个模块分别提供了装饰器，用于验证请求的用户是否已经登录/为管理员。  
* auto_test：自动评测模块，用于对模型在数据集上进行自动评测以及调用模型的url生成结果。  
* elo_rating：Elo评分模块，用于计算模型之间的Elo评分，用于模型测试的对手匹配。  
* send_msg：发送手机验证码接口，通过调用阿里云的短信服务，向用户发送验证码。  
* validation_error：自定义的异常类，用于处理前端传来的数据不合法的情况，能自定义返回信息。  
* verify_dataset：数据集验证模块，用于验证数据集文件的合法性。

## 重点和难点问题及其解决方法，以及核心算法  

### 问题与解决方案

+ Problem：登录过期需要全局提醒、退出登录并跳转到主页。

  Solution：利用mitt库创建事件总线，在项目入口文件中把路由变量router传递给axios拦截器的配置函数，当axios的response拦截器遇到jwt过期的error时，弹出全局提醒对话框，调用退出登录的函数，并通过router来切换到主页路由，同时借助事件总线通知相关组件重新渲染。

+ Problem：在性能测试中，我们针对包含图片/文件上传功能的部分接口进行了大并发量的上传请求模拟，发现在这一情况下管理好本地文件是一个较复杂问题。

  Solution：原本的接口仅做了最基本的处理，对于上传得到的文件全部保留，仅会在一个文件上传完成时更新文件路径，这造成大量冗余且无意义文件的出现，浪费服务器资源。作为一个朴素的修复方法，我们设置在新文件上传成功之后便立即删除旧文件。在低并发量下我们的代码工作的不错，但当并发量更高的时候仍然会产生很多冗余文件未被删除。经过问题分析发现，删除旧文件-创建新文件这一流程并不是原子操作，在接收并发请求的过程中可能出现多次删除同一旧文件，再更新多个新文件的问题。

  同时进行了多个处理：

  将一次删除和创建新文件放在一个原子操作中；

  删除改为使用os.remove进行，当原本文件不存在时可以抛出异常；

  对传入的文件用合适的方式重命名，消除了文件名自然冲突的可能。

  在这三个改动之后，即使是较高的并发量也能产生符合期望的结果，也不会浪费服务器资源。

+ Problem：文件上传功能的实现，前端组件采用了arco-pro-design的a-upload组件，但是教程中没有提供具体细节，网上也缺少相关示例。
  
  Solution：先是发现前端上传文件屡屡显示上传失败，在经过多次失败的尝试后，查看返回信息发现前端上传文件时会发出一个POST请求，但是这个请求并没有被我们定义，经过尝试后猜想与a-upload组件的action参数有关，在与后端协调后增加了一个无用的接口，将action参数修改为该接口的url，发现文件才能正确上传。之后查阅相关资料，发现上传文件还不能使用JSON格式，要使用formData结构体来存储fileItem对象，修改后实现了文件的上传功能，并在许多模块都得到了成功应用。


### 核心算法  
一个我们使用的算法是在模型对抗评测中涉及到的Elo评分算法，这一算法用于计算模型之间的实力差距，从而决定对抗评测中的对手匹配。 
Elo评分算法是一种用于计算棋手等级的算法，其基本思想是通过比赛结果来更新棋手的等级。 
我们的Elo评分算法的具体实现如下：  

1. 首先，每个模型有一个elo值，初始所有模型的elo值都为1500，这是推荐的初始值。  
2. 每次对抗评测结束后，我们会根据对抗评测的结果来更新模型的elo值，计算公式如下：  
```
ea = 1 / (1 + 10**((rb - ra) / 400))
eb = 1 / (1 + 10**((ra - rb) / 400))
k = 32
return ra + k * (sa - ea), rb + k * ((1 - sa) - eb)
```
这个算法的核心思想是，胜利带来的elo值增量与模型的实力差距成正比，如果模型A原本的elo值远高于模型B，那么模型A的elo值增加会很小，模型B的elo值减少会很小，反之亦然。  
## 测试总结

### 基础要求

#### 功能测试

项目的功能测试随着开发进程同步进行，执行测试的操作系统包括Windows和MacOS，通过Chrome浏览器执行。在完成主要功能点全部的功能开发后并合并分支后，会分别对这一功能点下的所有功能进行单独测试以及随即顺序的访问测试，如若发现问题则需进行bug修复。

此外，每次发布大版本之前也需要对以实现的全部功能进行功能测试，确保当前版本存在的问题不会带到下一版本。我们通过频繁且全面的功能测试，保证发布的应用中尽量不存在影响使用的显式bug。

#### 性能测试
采用了Apache JMeter进行性能测试，测试的目标涵盖了四个后端模块的各种接口。  
在较高的并发量下（100个线程），后端接口的响应时间也能保持在100ms以内。延迟时间变长的一个主要原因是，list需要传输的数据以及花费的时间明显变长。  
此外，针对有关文件上传的部分我们单独进行了处理，让文件在高并发量的情况下也能保持数据库中的正确更新。  
在高并发量下，性能测试的接口调用并不能保证100%的成功率（100并发），但这是由于我们对上述的文件处理导致的结果，如果发现了文件冲突则返回错误信息。除文件外的接口均工作完全正常。  

### 提高要求

#### 单元测试
单元测试是我们项目的一大亮点，我们的单元测试相当完备，这主要是由于后端的开发是测试驱动的：我们遵循写测试->写功能->完善测试的流程，让单元测试涵盖到各种项目中的易错点，在这一过程中也保障了后端接口的质量。  
后端接口在使用git上传之前都会在单元测试中进行测试，保证了接口的正确性之后再上传。  
我们还使用了coverage工具对单元测试的覆盖率进行了统计，覆盖率达到了97%，这也是我们的单元测试能够保证项目质量的重要原因。  

## 系统部署
### 项目结构简述  
项目部署基于docker，具体关系为数据库MySQL、后端Django(DRF)、代理服务器Nginx分别位于三个容器内，前两者与后两者分别连接，与小作业保持一致。 
项目中的主要配置文件均位于backend目录下，具体位置为（相对backend目录）：  

* requirements.txt：python依赖  
* Model_Valhalla/settings_prod.py：生产环境采用的配置（指定了MySQL等）  
* nginx/model_valhalla.conf：nginx配置文件  
* Dockerfile and docker-compose.yml：docker配置文件  
* .env：MySQL环境变量配置文件  

### 部署至服务器  
项目的服务器部署依靠docker一步实现，在backend目录下执行`docker-compose up`即可。 
注意，在部署前需要确保前端的静态产生文件已经正确提供。前端得到的静态文件在本项目中为assets文件夹以及一个index.html文件，这些文件应当位于backend/build目录下。 
